{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24885bf-4893-4f98-8956-4dff3737dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f0e6f1-57c1-4426-ba96-038eb26d5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when programmatically accessing spark, you need to create a session first\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e286f5b5-7bba-4c55-bb58-71f07e4778b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet data\n",
    "df = spark.read.parquet(\"./data/tripdata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e542465f-b5b8-44eb-938d-3c21e552b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .parquet is a self describing file format - it already contains a schema. Print the schema of the read file.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4034f8bf-9f45-4f27-8f4a-11a8dc3a4455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " VendorID              | 1                   \n",
      " tpep_pickup_datetime  | 2024-05-01 00:59:15 \n",
      " tpep_dropoff_datetime | 2024-05-01 01:23:50 \n",
      " passenger_count       | 1                   \n",
      " trip_distance         | 6.1                 \n",
      " RatecodeID            | 1                   \n",
      " store_and_fwd_flag    | N                   \n",
      " PULocationID          | 138                 \n",
      " DOLocationID          | 145                 \n",
      " payment_type          | 1                   \n",
      " fare_amount           | 28.2                \n",
      " extra                 | 7.75                \n",
      " mta_tax               | 0.5                 \n",
      " tip_amount            | 5.0                 \n",
      " tolls_amount          | 0.0                 \n",
      " improvement_surcharge | 1.0                 \n",
      " total_amount          | 42.45               \n",
      " congestion_surcharge  | 0.0                 \n",
      " Airport_fee           | 1.75                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the first record of the file\n",
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be3608f-c276-40dd-a308-f48c96db9b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3723833"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the total number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886ae6c7-4771-4c3c-a016-c3cc4d0362c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use SQL statements directly as well instead of pyspark functions\n",
    "# register the dataframe as a table first\n",
    "df.createOrReplaceTempView(\"dataTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27993916-9db9-4384-959c-bff4fe6159c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3723833|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now you can run the SQL statements\n",
    "query = \"SELECT count(*) FROM dataTable\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974420f-e910-41d8-a9a3-1707354f8d9c",
   "metadata": {},
   "source": [
    "### Average Revenue Per Vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b23a6bc-ea5b-45c7-9712-72899c7416f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|VendorID|avg(total_amount)|\n",
      "+--------+-----------------+\n",
      "|       1|27.77309818844534|\n",
      "|       2|28.56437778683938|\n",
      "|       6|44.27804444444447|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using pyspark functions\n",
    "df.select(df.VendorID, df.total_amount).groupby('VendorID').avg('total_amount').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "708e2501-a5f0-429e-98f4-16ea82ad7e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+\n",
      "|VendorID|avg(total_amount)|\n",
      "+--------+-----------------+\n",
      "|       1|27.77309818844534|\n",
      "|       2|28.56437778683938|\n",
      "|       6|44.27804444444447|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL query\n",
    "query = \"SELECT VendorID, avg(total_amount) FROM dataTable GROUP BY VendorID\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0163d2-c35b-46b9-b63b-61367c355dd8",
   "metadata": {},
   "source": [
    "### Trip distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee5720a5-2178-4ae4-aa5a-4b6a4d55d2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(trip_distance)|\n",
      "+------------------+\n",
      "| 5.367049832793204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data needs to be grouped before running any aggregate functions - running df.avg('column') will give an error\n",
    "df.groupBy().avg(\"trip_distance\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d11ae9-de8d-4d92-95d6-a9e41ae00652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|avg(trip_distance)|\n",
      "+------------------+\n",
      "| 5.367049832793204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT avg(trip_distance) FROM dataTable\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7cae62a-041c-4a19-bec7-13fcf56d861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|avg(trip_distance)|\n",
      "+--------+------------------+\n",
      "|       1|3.2256287155426717|\n",
      "|       2| 6.046133441822597|\n",
      "|       6|10.220799999999999|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.trip_distance).groupBy('VendorID').avg('trip_distance').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9223e5f7-641a-4495-9ddc-b9bae3a006ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|avg(trip_distance)|\n",
      "+--------+------------------+\n",
      "|       1|3.2256287155426717|\n",
      "|       2| 6.046133441822597|\n",
      "|       6|10.220799999999999|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT VendorID, avg(trip_distance) FROM dataTable GROUP BY VendorID'\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7363e-93a6-4c1e-9244-64c52a899d7f",
   "metadata": {},
   "source": [
    "### Pickup Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2394c0-cfed-4757-8a1d-66df0eab8ff5",
   "metadata": {},
   "source": [
    "#### Top 10 pickup locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb0f226-835d-47db-a59d-fde1adec38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|         237|183490|\n",
      "|         161|174675|\n",
      "|         236|168379|\n",
      "|         132|167546|\n",
      "|         138|127137|\n",
      "|         162|125417|\n",
      "|         142|124892|\n",
      "|         230|121996|\n",
      "|         186|114356|\n",
      "|         163|107303|\n",
      "+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.PULocationID).groupBy('PULocationID').count().sort(functions.desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2742704-285a-4993-ba3e-d0fe865cf25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|PULocationID| count|\n",
      "+------------+------+\n",
      "|         237|183490|\n",
      "|         161|174675|\n",
      "|         236|168379|\n",
      "|         132|167546|\n",
      "|         138|127137|\n",
      "|         162|125417|\n",
      "|         142|124892|\n",
      "|         230|121996|\n",
      "|         186|114356|\n",
      "|         163|107303|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT PULocationID, count(PULocationID) as count FROM dataTable GROUP BY PULocationID ORDER BY count(PULocationID) desc LIMIT 10\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4e7f4-f099-42d1-9f09-13b31dfb1e42",
   "metadata": {},
   "source": [
    "### Dropoff Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f055030e-1c98-4e91-8991-abdc0cc40217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DOLocationID| count|\n",
      "+------------+------+\n",
      "|         236|172072|\n",
      "|         237|165143|\n",
      "|         161|141823|\n",
      "|         230|119124|\n",
      "|         170|108171|\n",
      "|         162|106543|\n",
      "|         142|106372|\n",
      "|         239|102809|\n",
      "|          68| 95180|\n",
      "|         141| 94888|\n",
      "+------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.DOLocationID).groupBy('DOLocationID').count().sort(functions.desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7341f41-3726-4b1a-b117-6c9b55782e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DOLocationID| count|\n",
      "+------------+------+\n",
      "|         236|172072|\n",
      "|         237|165143|\n",
      "|         161|141823|\n",
      "|         230|119124|\n",
      "|         170|108171|\n",
      "|         162|106543|\n",
      "|         142|106372|\n",
      "|         239|102809|\n",
      "|          68| 95180|\n",
      "|         141| 94888|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT DOLocationID, count(DOLocationID) as count FROM dataTable GROUP BY DOLocationID ORDER BY count(DOLocationID) desc LIMIT 10\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c193951-4bf0-4121-a718-7336f86b6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------------+\n",
      "|BusyLocationsID|\n",
      "+---------------+\n",
      "|            237|\n",
      "|            161|\n",
      "|            236|\n",
      "|            162|\n",
      "|            142|\n",
      "|            230|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql.catalogImplementation('Hive')\n",
    "query1 = \"CREATE TABLE Top10PU AS SELECT PULocationID, count(PULocationID) as count FROM dataTable GROUP BY PULocationID ORDER BY count(PULocationID) desc LIMIT 10\"\n",
    "query2 = \"CREATE TABLE Top10DO AS SELECT DOLocationID, count(DOLocationID) as count FROM dataTable GROUP BY DOLocationID ORDER BY count(DOLocationID) desc LIMIT 10\"\n",
    "query3 = \"SELECT PULocationID as BusyLocationsID FROM Top10PU INNER JOIN Top10DO on Top10PU.PULocationID = Top10DO.DOLocationID\"\n",
    "spark.sql(query1)\n",
    "spark.sql(query2)\n",
    "spark.sql(query3).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
